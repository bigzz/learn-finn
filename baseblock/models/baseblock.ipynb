{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e50f4c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.quantization\n",
    "import brevitas.nn as qnn\n",
    "import brevitas.onnx as bo\n",
    "from brevitas.nn import QuantConv2d, QuantLinear, QuantReLU, QuantAvgPool2d\n",
    "from brevitas.quant import IntBias\n",
    "\n",
    "from brevitas.core.restrict_val import RestrictValueType\n",
    "from brevitas.quant import Uint8ActPerTensorFloatMaxInit, Int8ActPerTensorFloatMinMaxInit\n",
    "from brevitas.quant import Int8WeightPerTensorFloat\n",
    "\n",
    "\n",
    "class CommonIntWeightPerTensorQuant(Int8WeightPerTensorFloat):\n",
    "    \"\"\"\n",
    "    Common per-tensor weight quantizer with bit-width set to None so that it's forced to be\n",
    "    specified by each layer.\n",
    "    \"\"\"\n",
    "    scaling_min_val = 2e-16\n",
    "    bit_width = None\n",
    "\n",
    "\n",
    "class CommonIntWeightPerChannelQuant(CommonIntWeightPerTensorQuant):\n",
    "    \"\"\"\n",
    "    Common per-channel weight quantizer with bit-width set to None so that it's forced to be\n",
    "    specified by each layer.\n",
    "    \"\"\"\n",
    "    scaling_per_output_channel = True\n",
    "\n",
    "\n",
    "class CommonIntActQuant(Int8ActPerTensorFloatMinMaxInit):\n",
    "    \"\"\"\n",
    "    Common signed act quantizer with bit-width set to None so that it's forced to be specified by\n",
    "    each layer.\n",
    "    \"\"\"\n",
    "    scaling_min_val = 2e-16\n",
    "    bit_width = None\n",
    "    min_val = -10.0\n",
    "    max_val = 10.0\n",
    "    restrict_scaling_type = RestrictValueType.LOG_FP\n",
    "\n",
    "\n",
    "class CommonUintActQuant(Uint8ActPerTensorFloatMaxInit):\n",
    "    \"\"\"\n",
    "    Common unsigned act quantizer with bit-width set to None so that it's forced to be specified by\n",
    "    each layer.\n",
    "    \"\"\"\n",
    "    scaling_min_val = 2e-16\n",
    "    bit_width = None\n",
    "    max_val = 6.0\n",
    "    restrict_scaling_type = RestrictValueType.LOG_FP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c03053",
   "metadata": {},
   "source": [
    "### Conv\n",
    "\n",
    "```python\n",
    "class QuantizedConvNdFn(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def symbolic(\n",
    "            g, x, W, w_qnt_scale, b_qnt_scale, w_qnt_type, b_qnt_type, out_shape, pads, strides,\n",
    "            bias, kernel_shape, groups, dilations):\n",
    "        ret = g.op(\n",
    "            f'{DOMAIN_STRING}::Conv', x, W,\n",
    "            weight_qnt_s=w_qnt_type,\n",
    "            kernel_shape_i=kernel_shape,\n",
    "            pads_i=pads,\n",
    "            strides_i=strides,\n",
    "            group_i=groups,\n",
    "            dilations_i=dilations)\n",
    "        if w_qnt_scale is not None:\n",
    "            ret = g.op('Mul', ret, w_qnt_scale)\n",
    "        if bias is not None:\n",
    "            if b_qnt_type is not None:\n",
    "                assert b_qnt_scale is not None\n",
    "                ret = g.op('Div', ret, b_qnt_scale)\n",
    "                ret = g.op('{DOMAIN_STRING}::Add', ret, bias, bias_qnt_s=b_qnt_type)\n",
    "                ret = g.op('Mul', ret, b_qnt_scale)\n",
    "            else:\n",
    "                ret = g.op('Add', ret, bias)\n",
    "        return ret\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(\n",
    "            ctx, x, W, w_qnt_scale, b_qnt_scale, w_qnt_type, b_qnt_type, out_shape, pads, strides,\n",
    "            bias, kernel_shape, groups, dilations):\n",
    "        return torch.empty(out_shape, dtype=torch.float, device=x.device)\n",
    "```\n",
    "---\n",
    "- CASE 1: w_qnt_scale(weight_quant):     QuantizedConv =  Conv + Mul\n",
    "- CASE 2: if(bias and bias_quant):       QuantizedConv =  Conv + Mul + Div + Add + Mul\n",
    "- CASE 3: if(bias):                      QuantizedConv =  Conv + Mul + Add\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39d1a81e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvBlock(\n",
      "  (conv): QuantConv2d(\n",
      "    3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "    (input_quant): ActQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "    )\n",
      "    (output_quant): ActQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "    )\n",
      "    (weight_quant): WeightQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "      (tensor_quant): RescalingIntQuant(\n",
      "        (int_quant): IntQuant(\n",
      "          (float_to_int_impl): RoundSte()\n",
      "          (tensor_clamp_impl): TensorClampSte()\n",
      "          (delay_wrapper): DelayWrapper(\n",
      "            (delay_impl): _NoDelay()\n",
      "          )\n",
      "        )\n",
      "        (scaling_impl): StatsFromParameterScaling(\n",
      "          (parameter_list_stats): _ParameterListStats(\n",
      "            (first_tracked_param): _ViewParameterWrapper(\n",
      "              (view_shape_impl): OverTensorView()\n",
      "            )\n",
      "            (stats): _Stats(\n",
      "              (stats_impl): AbsMax()\n",
      "            )\n",
      "          )\n",
      "          (stats_scaling_impl): _StatsScaling(\n",
      "            (affine_rescaling): Identity()\n",
      "            (restrict_clamp_scaling): _RestrictClampValue(\n",
      "              (clamp_min_ste): Identity()\n",
      "              (restrict_value_impl): FloatRestrictValue()\n",
      "            )\n",
      "            (restrict_scaling_pre): Identity()\n",
      "          )\n",
      "        )\n",
      "        (int_scaling_impl): IntScaling()\n",
      "        (zero_point_impl): ZeroZeroPoint(\n",
      "          (zero_point): StatelessBuffer()\n",
      "        )\n",
      "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
      "          (bit_width): StatelessBuffer()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (bias_quant): BiasQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Model saved to conv_finn.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The shape inference of finn.custom_op.general::Conv type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of finn.custom_op.general::Conv type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of finn.custom_op.general::Conv type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n"
     ]
    }
   ],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, inp, outp, stride=1):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = QuantConv2d(inp, outp, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False, return_quant_tensor=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "model_for_export = \"conv.pth\"\n",
    "ready_model_filename = \"conv_finn.onnx\"\n",
    "\n",
    "model = ConvBlock(3,64)\n",
    "print(model)\n",
    "torch.save(model.state_dict(),model_for_export)\n",
    "input_shape = (1, 3, 224, 224)\n",
    "bo.export_finn_onnx(model, input_shape, export_path=ready_model_filename,)\n",
    "print(\"Model saved to %s\" % ready_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "239cc3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving 'conv_finn.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fe7143018e0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.util.visualization import showInNetron\n",
    "\n",
    "showInNetron(ready_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7320b025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2de0d1e3",
   "metadata": {},
   "source": [
    "### QuantReLU\n",
    "\n",
    "```python\n",
    "class QuantReLUFn(Function): \n",
    "\n",
    "    @staticmethod\n",
    "    def symbolic(g, input, qnt_type, thres, bias, scale):\n",
    "        ret = g.op(f'{DOMAIN_STRING}::MultiThreshold', input, thres, out_dtype_s=qnt_type)\n",
    "        if scale is not None:\n",
    "            ret = g.op('Mul', ret, scale)\n",
    "        return ret\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, qnt_type, thres, bias, scale):\n",
    "        return input.clamp(0)\n",
    "```\n",
    "---\n",
    "\n",
    "- CASE 1: QuantReLU = MultiThreshold \n",
    "- CASE 2: if scaleï¼Œ QuantReLU =  MultiThreshold + Mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3d9f03d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLUBlock(\n",
      "  (relu): QuantReLU(\n",
      "    (input_quant): ActQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "    )\n",
      "    (act_quant): ActQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "      (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
      "        (activation_impl): ReLU()\n",
      "        (tensor_quant): RescalingIntQuant(\n",
      "          (int_quant): IntQuant(\n",
      "            (float_to_int_impl): RoundSte()\n",
      "            (tensor_clamp_impl): TensorClamp()\n",
      "            (delay_wrapper): DelayWrapper(\n",
      "              (delay_impl): _NoDelay()\n",
      "            )\n",
      "          )\n",
      "          (scaling_impl): ParameterFromRuntimeStatsScaling(\n",
      "            (stats_input_view_shape_impl): OverTensorView()\n",
      "            (stats): _Stats(\n",
      "              (stats_impl): AbsPercentile()\n",
      "            )\n",
      "            (restrict_clamp_scaling): _RestrictClampValue(\n",
      "              (clamp_min_ste): Identity()\n",
      "              (restrict_value_impl): FloatRestrictValue()\n",
      "            )\n",
      "            (restrict_inplace_preprocess): Identity()\n",
      "            (restrict_preprocess): Identity()\n",
      "          )\n",
      "          (int_scaling_impl): IntScaling()\n",
      "          (zero_point_impl): ZeroZeroPoint(\n",
      "            (zero_point): StatelessBuffer()\n",
      "          )\n",
      "          (msb_clamp_bit_width_impl): BitWidthConst(\n",
      "            (bit_width): StatelessBuffer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Model saved to relu_finn.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The shape inference of finn.custom_op.general::MultiThreshold type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of finn.custom_op.general::MultiThreshold type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of finn.custom_op.general::MultiThreshold type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n"
     ]
    }
   ],
   "source": [
    "class ReLUBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReLUBlock, self).__init__()\n",
    "        self.relu = QuantReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.relu(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "model_for_export = \"relu.pth\"\n",
    "ready_model_filename = \"relu_finn.onnx\"\n",
    "\n",
    "model = ReLUBlock()\n",
    "print(model)\n",
    "torch.save(model.state_dict(),model_for_export)\n",
    "input_shape = (1, 64, 224, 224)\n",
    "bo.export_finn_onnx(model, input_shape, export_path=ready_model_filename,)\n",
    "print(\"Model saved to %s\" % ready_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab41428f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving 'relu_finn.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fe71430b9a0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.util.visualization import showInNetron\n",
    "\n",
    "showInNetron(ready_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f73092",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc2c2915",
   "metadata": {},
   "source": [
    "### QuantizedLinear\n",
    "\n",
    "```python\n",
    "class QuantizedLinearFn(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def symbolic(g, x, Wt, w_qnt_scale, b_qnt_scale, w_qnt_type, b_qnt_type, out_shape, bias):\n",
    "        ret = g.op(f'{DOMAIN_STRING}::MatMul', x, Wt, weight_qnt_s=w_qnt_type)\n",
    "        if w_qnt_scale is not None:\n",
    "            ret = g.op('Mul', ret, w_qnt_scale)\n",
    "        if bias is not None:\n",
    "            if b_qnt_type is not None:\n",
    "                assert b_qnt_scale is not None\n",
    "                ret = g.op('Div', ret, b_qnt_scale)\n",
    "                ret = g.op('{DOMAIN_STRING}::Add', ret, bias, bias_qnt_s=b_qnt_type)\n",
    "                ret = g.op('Mul', ret, b_qnt_scale)\n",
    "            else:\n",
    "                ret = g.op('Add', ret, bias)\n",
    "        return ret\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, Wt, w_qnt_scale, b_qnt_scale, w_qnt_type, b_qnt_type, out_shape, bias):\n",
    "        return torch.empty(out_shape, dtype=torch.float, device=x.device)\n",
    "```\n",
    "---\n",
    "\n",
    "- CASE 1:                                        QuantizedLinear = MatMul \n",
    "- CASE 2: if(weight_quant):                      QuantizedLinear = MatMul +  Mul\n",
    "- CASE 3: if(bias):                              QuantizedLinear = MatMul +  Add\n",
    "- CASE 4: if(bias and bias_quant):               QuantizedLinear = MatMul +  Div  +  Add  + Mul\n",
    "- CASE 5: if(weight and bias):                   QuantizedLinear = MatMul +  Mul  +  Add\n",
    "- CASE 8: if(weight and bias and bias_quant):    QuantizedLinear = MatMul +  Mul  +  Div  +  Add  + Mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc16d7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantizedLinearBlock(\n",
      "  (linear): QuantLinear(\n",
      "    in_features=1024, out_features=10, bias=True\n",
      "    (input_quant): ActQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "    )\n",
      "    (output_quant): ActQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "    )\n",
      "    (weight_quant): WeightQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "      (tensor_quant): RescalingIntQuant(\n",
      "        (int_quant): IntQuant(\n",
      "          (float_to_int_impl): RoundSte()\n",
      "          (tensor_clamp_impl): TensorClampSte()\n",
      "          (delay_wrapper): DelayWrapper(\n",
      "            (delay_impl): _NoDelay()\n",
      "          )\n",
      "        )\n",
      "        (scaling_impl): StatsFromParameterScaling(\n",
      "          (parameter_list_stats): _ParameterListStats(\n",
      "            (first_tracked_param): _ViewParameterWrapper(\n",
      "              (view_shape_impl): OverTensorView()\n",
      "            )\n",
      "            (stats): _Stats(\n",
      "              (stats_impl): AbsMax()\n",
      "            )\n",
      "          )\n",
      "          (stats_scaling_impl): _StatsScaling(\n",
      "            (affine_rescaling): Identity()\n",
      "            (restrict_clamp_scaling): _RestrictClampValue(\n",
      "              (clamp_min_ste): Identity()\n",
      "              (restrict_value_impl): FloatRestrictValue()\n",
      "            )\n",
      "            (restrict_scaling_pre): Identity()\n",
      "          )\n",
      "        )\n",
      "        (int_scaling_impl): IntScaling()\n",
      "        (zero_point_impl): ZeroZeroPoint(\n",
      "          (zero_point): StatelessBuffer()\n",
      "        )\n",
      "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
      "          (bit_width): StatelessBuffer()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (bias_quant): BiasQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Model saved to linear_finn.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The shape inference of finn.custom_op.general::MatMul type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of finn.custom_op.general::MatMul type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of finn.custom_op.general::MatMul type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n"
     ]
    }
   ],
   "source": [
    "class QuantizedLinearBlock(nn.Module):\n",
    "    def __init__(self, inp, outp):\n",
    "        super(QuantizedLinearBlock, self).__init__()\n",
    "        self.linear = QuantLinear(inp, outp, bias=True, return_quant_tensor=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "model_for_export = \"linear.pth\"\n",
    "ready_model_filename = \"linear_finn.onnx\"\n",
    "\n",
    "model = QuantizedLinearBlock(1024,10)\n",
    "print(model)\n",
    "torch.save(model.state_dict(),model_for_export)\n",
    "input_shape = (1, 1024)\n",
    "bo.export_finn_onnx(model, input_shape, export_path=ready_model_filename,)\n",
    "print(\"Model saved to %s\" % ready_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d8c99a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving 'linear_finn.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fe71605d7c0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.util.visualization import showInNetron\n",
    "\n",
    "showInNetron(ready_model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e227d3e",
   "metadata": {},
   "source": [
    "### QuantIdentity\n",
    "\n",
    "- QuantIdentity = MultiThreshold + Add + Mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a8ea96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantIdentityBlock(\n",
      "  (quant_inp): QuantIdentity(\n",
      "    (input_quant): ActQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "    )\n",
      "    (act_quant): ActQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "      (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
      "        (activation_impl): Identity()\n",
      "        (tensor_quant): RescalingIntQuant(\n",
      "          (int_quant): IntQuant(\n",
      "            (float_to_int_impl): RoundSte()\n",
      "            (tensor_clamp_impl): TensorClamp()\n",
      "            (delay_wrapper): DelayWrapper(\n",
      "              (delay_impl): _NoDelay()\n",
      "            )\n",
      "          )\n",
      "          (scaling_impl): ParameterFromRuntimeStatsScaling(\n",
      "            (stats_input_view_shape_impl): OverTensorView()\n",
      "            (stats): _Stats(\n",
      "              (stats_impl): AbsPercentile()\n",
      "            )\n",
      "            (restrict_clamp_scaling): _RestrictClampValue(\n",
      "              (clamp_min_ste): Identity()\n",
      "              (restrict_value_impl): FloatRestrictValue()\n",
      "            )\n",
      "            (restrict_inplace_preprocess): Identity()\n",
      "            (restrict_preprocess): Identity()\n",
      "          )\n",
      "          (int_scaling_impl): IntScaling()\n",
      "          (zero_point_impl): ZeroZeroPoint(\n",
      "            (zero_point): StatelessBuffer()\n",
      "          )\n",
      "          (msb_clamp_bit_width_impl): BitWidthConst(\n",
      "            (bit_width): StatelessBuffer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Model saved to identity_finn.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The shape inference of finn.custom_op.general::MultiThreshold type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of finn.custom_op.general::MultiThreshold type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of finn.custom_op.general::MultiThreshold type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n"
     ]
    }
   ],
   "source": [
    "class QuantIdentityBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QuantIdentityBlock, self).__init__()\n",
    "        self.quant_inp = qnn.QuantIdentity(bit_width=8, return_quant_tensor=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out= self.quant_inp(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "model_for_export = \"identity.pth\"\n",
    "ready_model_filename = \"identity_finn.onnx\"\n",
    "\n",
    "model = QuantIdentityBlock()\n",
    "print(model)\n",
    "torch.save(model.state_dict(),model_for_export)\n",
    "input_shape = (1,3,320,320)\n",
    "bo.export_finn_onnx(model, input_shape, export_path=ready_model_filename)\n",
    "print(\"Model saved to %s\" % ready_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6af0c588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving 'identity_finn.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fe714153760>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.util.visualization import showInNetron\n",
    "\n",
    "showInNetron(ready_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fe9fda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2eb217aa",
   "metadata": {},
   "source": [
    "### QuantAvgPool2d\n",
    "\n",
    "```python\n",
    "class QuantAvgPool2dFn(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def symbolic(g, x, out_shape, kernel, stride, signed, ibits, obits, scale, qnt_type):\n",
    "        if scale is not None:\n",
    "            x = g.op('{DOMAIN_STRING}::Div', x, scale, activation_qnt_s=qnt_type)\n",
    "        ret = g.op(\n",
    "            f'{DOMAIN_STRING}::QuantAvgPool2d', x,\n",
    "            kernel_i=kernel,\n",
    "            stride_i=stride,\n",
    "            signed_i=signed,\n",
    "            ibits_i=ibits,\n",
    "            obits_i=obits)\n",
    "        if scale is not None:\n",
    "            ret = g.op('Mul', ret, scale)\n",
    "        return ret\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, out_shape, kernel, stride, signed, ibits, obits, scale, qnt_type):\n",
    "        return torch.empty(out_shape, dtype=torch.float, device=x.device)\n",
    "    \n",
    "```\n",
    "---\n",
    "\n",
    "- CASE 1: if(scale):                             QuantAvgPool2d = Div + QuantAvgPool2d + Mul\n",
    "- CASE 2:                                        QuantAvgPool2d = QuantAvgPool2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "958fc039",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuantAvgPoolBlock(\n",
      "  (quant_inp): QuantIdentity(\n",
      "    (input_quant): ActQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "    )\n",
      "    (act_quant): ActQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "      (fused_activation_quant_proxy): FusedActivationQuantProxy(\n",
      "        (activation_impl): Identity()\n",
      "        (tensor_quant): RescalingIntQuant(\n",
      "          (int_quant): IntQuant(\n",
      "            (float_to_int_impl): RoundSte()\n",
      "            (tensor_clamp_impl): TensorClamp()\n",
      "            (delay_wrapper): DelayWrapper(\n",
      "              (delay_impl): _NoDelay()\n",
      "            )\n",
      "          )\n",
      "          (scaling_impl): ParameterFromRuntimeStatsScaling(\n",
      "            (stats_input_view_shape_impl): OverTensorView()\n",
      "            (stats): _Stats(\n",
      "              (stats_impl): AbsPercentile()\n",
      "            )\n",
      "            (restrict_clamp_scaling): _RestrictClampValue(\n",
      "              (clamp_min_ste): Identity()\n",
      "              (restrict_value_impl): FloatRestrictValue()\n",
      "            )\n",
      "            (restrict_inplace_preprocess): Identity()\n",
      "            (restrict_preprocess): Identity()\n",
      "          )\n",
      "          (int_scaling_impl): IntScaling()\n",
      "          (zero_point_impl): ZeroZeroPoint(\n",
      "            (zero_point): StatelessBuffer()\n",
      "          )\n",
      "          (msb_clamp_bit_width_impl): BitWidthConst(\n",
      "            (bit_width): StatelessBuffer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool): QuantAvgPool2d(\n",
      "    kernel_size=3, stride=1, padding=0\n",
      "    (trunc_quant): TruncQuantProxyFromInjector(\n",
      "      (_zero_hw_sentinel): StatelessBuffer()\n",
      "      (tensor_quant): TruncIntQuant(\n",
      "        (msb_clamp_bit_width_impl): BitWidthConst(\n",
      "          (bit_width): StatelessBuffer()\n",
      "        )\n",
      "        (float_to_int_impl): FloorSte()\n",
      "        (delay_wrapper): DelayWrapper(\n",
      "          (delay_impl): _NoDelay()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Model saved to pool_finn.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The shape inference of finn.custom_op.general::MultiThreshold type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of {DOMAIN_STRING}::Div type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of finn.custom_op.general::QuantAvgPool2d type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of finn.custom_op.general::MultiThreshold type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of {DOMAIN_STRING}::Div type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of finn.custom_op.general::QuantAvgPool2d type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of finn.custom_op.general::MultiThreshold type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of {DOMAIN_STRING}::Div type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n",
      "WARNING: The shape inference of finn.custom_op.general::QuantAvgPool2d type is missing, so it may result in wrong shape inference for the exported graph. Please consider adding it in symbolic function.\n"
     ]
    }
   ],
   "source": [
    "class QuantAvgPoolBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QuantAvgPoolBlock, self).__init__()\n",
    "        self.quant_inp = qnn.QuantIdentity(bit_width=8, return_quant_tensor=True)\n",
    "        self.pool = QuantAvgPool2d(kernel_size=3, stride=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x= self.quant_inp(x)\n",
    "        out = self.pool(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "model_for_export = \"pool.pth\"\n",
    "ready_model_filename = \"pool_finn.onnx\"\n",
    "\n",
    "model = QuantAvgPoolBlock()\n",
    "print(model)\n",
    "torch.save(model.state_dict(),model_for_export)\n",
    "input_shape = (1,3,320,320)\n",
    "bo.export_finn_onnx(model, input_shape, export_path=ready_model_filename)\n",
    "print(\"Model saved to %s\" % ready_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c641ad25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving 'pool_finn.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fe7165f1ca0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.util.visualization import showInNetron\n",
    "\n",
    "showInNetron(ready_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a19afa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
